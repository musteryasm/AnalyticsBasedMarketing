# -*- coding: utf-8 -*-
"""Marketing_Campaign.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qTX_5AQwpice4QosBQOxibuEbVYc8fhd

###Importing data
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/1_LiveProjects/Project4_Analytics_Enabled_Marketing
# !pwd

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.linear_model import LogisticRegression

dataset=pd.read_excel("/content/drive/MyDrive/MLminiproject/a1_Dataset_10Percent.xlsx")
# !ls

# shows count of rows and columns
dataset.shape

#shows first few rows of the code
dataset.head()

"""### Data preprocessing"""

#dropping customer ID column from the dataset

dataset=dataset.drop(['ID'],axis=1)

dataset.head()

# explore missing values

dataset.isna().sum()

# filling missing values with mean/mode*

dataset['DemAffl']=dataset['DemAffl'].fillna(dataset['DemAffl'].mode()[0])
dataset['DemAge']=dataset['DemAge'].fillna(dataset['DemAge'].mode()[0])
dataset['DemClusterGroup']=dataset['DemClusterGroup'].fillna(dataset['DemClusterGroup'].mode()[0])
dataset['DemGender']=dataset['DemGender'].fillna(dataset['DemGender'].mode()[0])
dataset['DemReg']=dataset['DemReg'].fillna(dataset['DemReg'].mode()[0])
dataset['DemTVReg']=dataset['DemTVReg'].fillna(dataset['DemTVReg'].mode()[0])
dataset['LoyalTime']=dataset['LoyalTime'].fillna(dataset['LoyalTime'].mean())

# explore missing values post missing value fix

dataset.isna().sum()

dataset.head()

"""###Coverting category to numeric"""

# converting to mumeric

from sklearn.preprocessing import LabelEncoder
number = LabelEncoder()

dataset['DemClusterGroup'] = number.fit_transform(dataset['DemClusterGroup'].astype('str'))
integer_mapping = {l: i for i, l in enumerate(number.classes_)}
print(integer_mapping)

dataset['DemGender'] = number.fit_transform(dataset['DemGender'].astype('str'))
integer_mapping = {l: i for i, l in enumerate(number.classes_)}
print(integer_mapping)

dataset['DemReg'] = number.fit_transform(dataset['DemReg'].astype('str'))
integer_mapping = {l: i for i, l in enumerate(number.classes_)}
print(integer_mapping)

dataset['DemTVReg'] = number.fit_transform(dataset['DemTVReg'].astype('str'))
integer_mapping = {l: i for i, l in enumerate(number.classes_)}
print(integer_mapping)

dataset['LoyalClass'] = number.fit_transform(dataset['LoyalClass'].astype('str'))
integer_mapping = {l: i for i, l in enumerate(number.classes_)}
print(integer_mapping)

dataset.head()

"""###Checking for Multicollinearity

We check for multicollinearity in classifiers to ensure that the predictor variables are not highly correlated with each other, which can negatively affect the performance of the model. High multicollinearity can cause unstable or biased estimates of the coefficients of the predictor variables, which in turn can make it difficult to interpret the importance of each variable in the model.

A commonly used measure for multicollinearity is the variance inflation factor (VIF). The VIF measures the amount of correlation between a predictor variable and the other variables in the model, and a VIF value greater than 10 is often considered as an indication of high multicollinearity. However, the specific threshold for what is considered high multicollinearity can vary depending on the context and the field of study.

In general, a lower VIF value indicates lower multicollinearity, which leads to more stable and accurate estimates of the coefficients and better predictive performance of the model. Thus, it is important to check for multicollinearity and try to reduce it by either removing correlated variables or transforming the data before fitting the classifier model.
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(z):

    # Calculating VIF
    vif = pd.DataFrame()
    vif["variables"] = z.columns
    vif["VIF"] = [variance_inflation_factor(z.values, i) for i in range(z.shape[1])]

    return(vif)

z = dataset.iloc[:,0:9]
calc_vif(z)

"""### Variable selection"""

y = dataset.iloc[:, 9].values
X = dataset.iloc[:, 0:9].values

# splitting dataset into training and test (in ratio 80:20)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

"""EDA"""

import seaborn as sns
import matplotlib.pyplot as plt

"""Each scatterplot shows the relationship between two variables, and the diagonal plots show the distribution of each variable. This can be useful for identifying any patterns or correlations in the data, which can help inform data-driven decision-making. For example, we can see if there are any strong positive or negative correlations between different variables, or if there are any outliers or unusual patterns in the data."""

# Visualizing the data
sns.pairplot(dataset) # Plotting pairwise relationships in the dataset
plt.show()

"""This plot can help in identifying which loyalty program tiers and customer groups are the most profitable, as well as identifying potential differences in loyalty spend patterns between customers who made a purchase and those who did not. The use of hue parameter in the seaborn boxplot function allows us to compare and contrast the two categories of TargetBuy (made a purchase or not) within each level of LoyalClass, providing additional insights into the spending behavior of different customer groups."""

plt.figure(figsize=(10,15))
sns.boxplot(data=dataset, x='LoyalClass', y='LoyalSpend', hue='TargetBuy')
plt.xlabel('Loyalty Classification')
plt.ylabel('Loyalty Spend')
plt.title('Loyalty Spend Distribution by Classification')
plt.legend(title='Target Buy', loc='upper right')
plt.show()

"""The relationship between customer loyalty classification (LoyalClass) and loyalty spend (LoyalSpend) can be investigated by creating a box plot that shows the distribution of loyalty spend for each loyalty classification. This plot can help in identifying which loyalty program tiers are the most profitable and can inform decisions about how to structure and incentivize loyalty programs. Understanding this relationship is crucial as it can inform targeted marketing strategies to increase loyalty program participation and ultimately drive revenue."""

loyal_spend_means = dataset.groupby('LoyalClass')['LoyalSpend'].mean()
plt.bar(loyal_spend_means.index, loyal_spend_means.values)
plt.xlabel('Loyalty Classification')
plt.ylabel('Mean Loyalty Spend')
plt.title('Mean Loyalty Spend by Classification')
plt.show()

"""By analyzing the age and demographic affiliation of customers who made a purchase, we can tailor marketing campaigns towards specific age and demographic groups that are more likely to make a purchase. A histogram can be created to show the age distribution for customers who made a purchase and those who did not. Additionally, the relationship between demographic affiliation and whether or not a customer made a purchase can be explored to target certain demographic groups that are more likely to make a purchase."""

sns.histplot(data=dataset, x='DemAge')
plt.title('Distribution of DemAge')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

"""Analyzing customer loyalty time (LoyalTime) can help develop targeted retention strategies to keep customers engaged and coming back for more. To do this, we can create a histogram of the LoyalTime distribution for customers who made a purchase and those who did not. Additionally, we can analyze the distribution of DemAge to see if there are any age groups more likely to make a purchase, and explore the relationship between DemAffl and TargetBuy to help target marketing campaigns towards certain demographic groups. Furthermore, we can investigate the relationship between LoyalClass and LoyalSpend to identify profitable loyalty program tiers and structure loyalty programs accordingly."""

# Create histograms of loyalty time for customers who made a purchase and those who did not
sns.histplot(data=dataset, x='LoyalTime', hue='TargetBuy', multiple='stack', bins=10)
plt.xlabel('Loyalty Time(Years)')
plt.ylabel('Count')
plt.title('Loyalty Time Distribution by Purchase')
plt.show()

"""To investigate the relationship between DemAffl and TargetBuy, a stacked bar plot can be created to visualize the proportion of purchases made by each demographic affiliation group. This plot can be useful in targeting marketing campaigns towards certain demographic groups that are more likely to make a purchase, allowing for more effective marketing strategies. The relationship between DemAffl (demographic affiliation) and TargetBuy (customer purchase behavior) can provide valuable insights into consumer behavior and preferences, allowing businesses to tailor their marketing messages to better resonate with their target audience."""

affl_counts = dataset.groupby('DemAffl')['TargetBuy'].value_counts(normalize=True).mul(100)

affl_counts.unstack().plot(kind='bar', stacked=True)
plt.xlabel('Demographic Affiliation')
plt.ylabel('Proportion of Purchases (%)')
plt.title('Proportion of Purchases by Demographic Affiliation')
plt.show()

"""### Modelling"""

classifier =  LogisticRegression(max_iter=300)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

# Exporting Logistic Regression Classifier to later use in prediction
import joblib
joblib.dump(classifier, './c2_Classifier_LoyalCustomers')

print(confusion_matrix(y_test,y_pred))

print(accuracy_score(y_test, y_pred))

predictions = classifier.predict_proba(X_test)
predictions

# writing model output file

df_prediction_prob = pd.DataFrame(predictions, columns = ['prob_0', 'prob_1'])
df_test_dataset = pd.DataFrame(y_test,columns= ['Actual Outcome'])
df_x_test = pd.DataFrame(X_test)

dfx=pd.concat([df_x_test,df_test_dataset, df_prediction_prob], axis=1)

dfx.to_excel("c1_ModelOutput_10Percent.xlsx")

dfx.head()